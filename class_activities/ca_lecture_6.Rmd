---
title: "Class Activity, September 2"
output: 
  tufte::tufte_html:
    css: "lab.css"
    tufte_variant: "envisioned"
    highlight: pygments
link-citations: yes
---

In this class activity, you will practice maximum likelihood estimation. In Part I, you will use R to approximate the maximum likelihood estimate. In Part II, you will compute an exact solution with calculus.

# Part I

Suppose we have a variable $Y$ that can take **three** possible outcomes: $Y = -1, 0$, or $1$. We also know that

* $P(Y_i = 0) = \pi_0$
* $P(Y_i = -1) = 2 \pi_0$
* $P(Y_i = 1) = 1 - 3 \pi_0$

where the parameter $\pi_0$ is unknown. We observe data $-1, -1, 0, 1, 0, -1$, and we want to estimate $\pi_0$ from the data.

1. Let $\widehat{\pi}_0$ be an estimate of $\pi_0$. Write down the likelihood $L(\widehat{\pi}_0)$ for the observed data $-1, -1, 0, 1, 0, -1$.

2. Modify the simulation code from last class to calculate $L(\widehat{\pi}_0)$ for $\widehat{\pi}_0 = 0, 0.05, 0.1, 0.15, ..., 0.9, 0.95, 1$. For reference, the code from the slides is below. For what value of $\widehat{\pi}_0$ is the likelihood the highest?

```r
# List the values for pi hat
pi_hat <- seq(from = 0, to = 1, by = 0.1)

# Create a space to store the likelihoods
likelihood <- rep(0,length(pi_hat))

# Compute and store the likelihoods
for( i in 1:length(pi_hat) ){
  likelihood[i] <- pi_hat[i]*(1-pi_hat[i])^4
}

# print the results
likelihood
```


# Part II

3. Use calculus to maximize the likelihood from Exercise 1. Remember to differentiate the log likelihood, and recall the following rules for logs and derivatives:

* $\log(xy) = \log(x) + \log(y)$
* $\log(x^y) = y \log(x)$
* $\dfrac{d}{dx} \log x = \dfrac{1}{x}$
* $\dfrac{d}{dx} c f(x) = c \dfrac{d}{dx} f(x)$ for constant $c$
* $\dfrac{d}{dx} (f(x) + c) = \dfrac{d}{dx} f(x)$ for constant $c$
* $\dfrac{d}{dx} (f(x) + g(x)) = \dfrac{d}{dx} f(x) + \dfrac{d}{dx} g(x)$