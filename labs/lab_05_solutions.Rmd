---
title: "Lab 5 Solutions"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=F, warning=F)
library(tidyverse)
library(MASS)
library(gamair)
data("chicago")
```

### Question 1 (2 pts)

```{r}
chicago %>%
  ggplot(aes(x = death)) +
  geom_histogram() +
  labs(x = "Deaths") +
  theme_bw()
```

The number of deaths appears unimodal and slightly right skewed, with a few potential outliers above 200. A Poisson distribution seems like it could be a reasonable choice.

### Question 2 (2 pts)

```{r}
mean(chicago$death)
var(chicago$death)
```

The variance is more than twice the mean, so we may be worried about overdispersion.

### Question 3 (2 pts)

```{r include=F}
logmean_plot <- function(data, num_bins, bin_method,
                         x, y, grouping = NULL, reg_formula = y ~ x){
  
  if(is.null(grouping)){
    dat <- data.frame(x = data[,x], 
                      y = data[,y],
                      group = 1)
  } else {
    dat <- data.frame(x = data[,x], 
                      y = data[,y],
                      group = data[,grouping])
  }
  
  if(bin_method == "equal_size"){
    log_table <- dat %>%
      drop_na() %>%
      arrange(group, x) %>%
      group_by(group) %>%
      mutate(obs = y,
             bin = rep(1:num_bins,
                       each=ceiling(n()/num_bins))[1:n()]) %>%
      group_by(bin, group) %>%
      summarize(mean_x = mean(x),
                mean_y = mean(obs),
                num_obs = n()) %>%
      ungroup() %>%
      mutate(log_mean = log(mean_y))
  } else {
    log_table <- dat %>%
      drop_na() %>%
      group_by(group) %>%
      mutate(obs = y,
             bin = cut(x, 
                       breaks = num_bins,
                       labels = F)) %>%
      group_by(bin, group) %>%
      summarize(mean_x = mean(x),
                mean_y = mean(obs),
                num_obs = n()) %>%
      ungroup() %>%
      mutate(log_mean = log(mean_y))
  }
  
  if(is.null(grouping)){
    log_table %>%
      ggplot(aes(x = mean_x,
                 y = log_mean)) +
      geom_point(size=2.5) +
      geom_smooth(se=F, method="lm", formula = reg_formula) +
      theme_bw() +
      labs(x = x,
           y = "Empirical log mean count") +
      theme(text = element_text(size=25))
  } else {
    log_table %>%
      ggplot(aes(x = mean_x,
                 y = log_mean,
                 color = group,
                 shape = group)) +
      geom_point(size=2.5) +
      geom_smooth(se=F, method="lm", formula = reg_formula) +
      theme_bw() +
      labs(x = x,
           y = "Empirical log mean count",
           color = grouping,
           shape = grouping) +
      theme(text = element_text(size=25))
  }
  
}

```

```{r}
logmean_plot(chicago, 20, "equal_size", x = "tmpd",
             y = "death")
```

There may be a bit of nonlinearity in the relationship between temperature and the log mean number of deaths. I think a linear relationship is probably good enough for now, but we could use a polynomial model instead.


### Question 4 (2 pts)

We use an offset when counts come from different time periods, or from groups of different sizes. In this case, each number of deaths comes from a 24 hour period, so we do not need an offset to account for different lengths of time (note that `time` should *not* be used for an offset here). However, the study ranges from 1987 to 2000, and the population of Chicago probably changed during that time. So, if we had the daily population of Chicago for each day in the data, log(population) would make a good offset. But this information isn't available in the data.

**Grading:** Lose 1 pt if they say `time` should be an offset. Lose 1 pt if they suggest population as an offset, but don't realize it isn't in the data.


### Question 5 (2 pts)

$$Deaths_i \sim Poisson(\lambda_i)$$

$$\log(\lambda_i) = \beta_0 + \beta_1 Temperature_i$$

**Grading:** Lose 1 pt for incorrect notation

### Question 6 (3 pts)

```{r}
m1 <- glm(death ~ tmpd, data = chicago, family=poisson)
summary(m1)
```

$\log(\widehat{\lambda}_i) = 4.873 - 0.0025 \ Temperature_i$

We estimate that an increase of 1 degree in temperature is associated with a decrease in the average number of deaths by a factor of $e^{-0.0025} = 0.9975$.

**Grading:** Lose 1 pt if they interpret on the log scale and don't exponentiate

### Question 7 (3 pts)

The residual deviance is 8471.8, with 5112 degrees of freedom. If the model were a good fit to the data, the residual deviance would come from a $\chi^2_{5112}$ distribution. The corresponding p-value is

```{r}
pchisq(8471.8, df=5112, lower.tail=F)
```

which is approximately 0, so the model is not a good fit to the data. (This could occur because of overdispersion, or because we need to add more variables to the model)

### Question 8 (5 pts)

$Y_i \sim NB(\theta, p)$, so $P(Y_i = y) = \dfrac{(y + \theta - 1)!}{y!(\theta - 1)!} (1 - p)^\theta p^y$.

We observe $Y_1,...,Y_n$, so the likelihood is

$L(\widehat{p}) = \prod \limits_{i=1}^n \dfrac{(Y_i + \theta - 1)!}{Y_i!(\theta - 1)!} (1 - \widehat{p})^\theta \widehat{p}^Y_i$

Thus the log likelihood is

$\log L(\widehat{p}) = \sum \limits_{i=1}^n \left( \log \left( \dfrac{(Y_i + \theta - 1)!}{Y_i!(\theta - 1)!} \right) + \theta \log(1 - \widehat{p}) + Y_i \log(\widehat{p}) \right)$

We take the derivative and set equal to 0:

$\dfrac{d}{d\widehat{p}} \log L(\widehat{p}) = \sum \limits_{i=1}^n \left( -\dfrac{\theta}{1 - \widehat{p}} + \dfrac{Y_i}{\widehat{p}} \right) \overset{set}{=} 0$

So,

$\sum \limits_{i=1}^n \dfrac{Y_i}{\widehat{p}} = \sum \limits_{i=1}^n \dfrac{\theta}{1 - \widehat{p}} = \dfrac{n \theta}{1 - \widehat{p}}$

Rearranging,

$\dfrac{1 - \widehat{p}}{\widehat{p}} = \dfrac{n \theta}{\sum \limits_{i=1}^n Y_i}$

so

$\dfrac{1}{\widehat{p}} = \dfrac{n \theta + \sum \limits_{i=1}^n Y_i}{\sum \limits_{i=1}^n Y_i}$

and thus

$\widehat{p} = \dfrac{\sum \limits_{i=1}^n Y_i}{n\theta + \sum \limits_{i=1}^n Y_i}$
